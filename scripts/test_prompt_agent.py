"""Test the Dynamic Prompt Engineering Agent with different property types."""

import json
import os
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from parcel_ai_json.agents import PromptEngineeringAgent
from parcel_ai_json.agents.prompt_engineer import PropertyContext


def test_prompt_agent():
    """Test prompt generation for different property scenarios."""
    print("=" * 100)
    print("DYNAMIC PROMPT ENGINEERING AGENT - DEMONSTRATION")
    print("=" * 100)

    # Check if API key is available
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        print("\n‚ö†Ô∏è  WARNING: ANTHROPIC_API_KEY not set - using default prompts")
        print("Set env var to see dynamic prompt generation:\n")
        print("export ANTHROPIC_API_KEY='your-api-key'\n")

    # Initialize agent
    agent = PromptEngineeringAgent(api_key=api_key)

    # Test scenarios
    scenarios = [
        {
            "name": "Luxury Estate - Arizona",
            "context": PropertyContext(
                center_lat=33.5186,  # Scottsdale, AZ
                center_lon=-111.9004,
                lot_size_sqft=87120,  # 2 acres
                property_type="residential",
                climate_zone="warm",
            ),
        },
        {
            "name": "Urban Lot - Northeast",
            "context": PropertyContext(
                center_lat=42.3601,  # Boston, MA
                center_lon=-71.0589,
                lot_size_sqft=5000,  # ~0.11 acres
                property_type="residential",
                climate_zone="cold",
            ),
        },
        {
            "name": "Suburban Home - California",
            "context": PropertyContext(
                center_lat=33.0406,  # Ramona, CA (our test case)
                center_lon=-116.8669,
                lot_size_sqft=21780,  # 0.5 acres
                property_type="residential",
                climate_zone="warm",
            ),
        },
        {
            "name": "Rural Property - Pacific Northwest",
            "context": PropertyContext(
                center_lat=47.6062,  # Seattle area
                center_lon=-122.3321,
                lot_size_sqft=130680,  # 3 acres
                property_type="residential",
                climate_zone="temperate",
            ),
        },
    ]

    results = []

    for scenario in scenarios:
        print(f"\n{'-' * 100}")
        print(f"SCENARIO: {scenario['name']}")
        print("-" * 100)

        # Generate prompts
        result = agent.generate_prompts(scenario["context"], max_prompts=15)

        # Display results
        print(f"\nüìç Property Context:")
        print(f"   Location: {scenario['context'].center_lat:.4f}, "
              f"{scenario['context'].center_lon:.4f}")
        print(f"   Lot Size: {scenario['context'].lot_size_sqft:,} sqft "
              f"({scenario['context'].lot_size_sqft / 43560:.2f} acres)")
        print(f"   Climate: {scenario['context'].climate_zone}")

        print(f"\nüéØ Generated Prompts ({len(result.prompts)}):")
        for i, prompt in enumerate(result.prompts, 1):
            print(f"   {i:2d}. {prompt}")

        if result.excluded_prompts:
            print(f"\n‚ùå Excluded Prompts ({len(result.excluded_prompts)}):")
            for i, prompt in enumerate(result.excluded_prompts, 1):
                print(f"   {i:2d}. {prompt}")

        print(f"\nüí° Agent Reasoning:")
        print(f"   {result.reasoning}")

        print(f"\nüå°Ô∏è  Climate Analysis:")
        print(f"   {result.climate_context}")

        print(f"\nüèòÔ∏è  Property Analysis:")
        print(f"   {result.property_analysis}")

        print(f"\n‚úÖ Confidence Score: {result.confidence_score:.2f}")

        # Store for comparison
        results.append({
            "scenario": scenario["name"],
            "prompts": result.prompts,
            "reasoning": result.reasoning,
            "confidence": result.confidence_score,
            "climate": result.climate_context,
            "analysis": result.property_analysis,
        })

    # Save results to JSON
    output_dir = Path("output/examples/agent_tests")
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "prompt_agent_results.json"

    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n{'=' * 100}")
    print(f"‚úÖ RESULTS SAVED: {output_file}")
    print("=" * 100)

    # Compare prompts between scenarios
    print(f"\n{'=' * 100}")
    print("COMPARISON: Default vs Agent-Generated Prompts")
    print("=" * 100)

    default_prompts = set(PromptEngineeringAgent.DEFAULT_PROMPTS[:15])

    for result_data in results:
        agent_prompts = set(result_data["prompts"])

        unique_to_agent = agent_prompts - default_prompts
        unique_to_default = default_prompts - agent_prompts

        print(f"\nScenario: {result_data['scenario']}")
        if unique_to_agent:
            print(f"  üÜï New prompts (agent-specific):")
            for prompt in unique_to_agent:
                print(f"     - {prompt}")
        if unique_to_default:
            print(f"  üîÑ Replaced default prompts:")
            for prompt in unique_to_default:
                print(f"     - {prompt}")

    print(f"\n{'=' * 100}")
    print("SUMMARY")
    print("=" * 100)

    if api_key:
        print("\n‚úÖ Dynamic prompt generation: ACTIVE")
        print("   - Prompts adapted to property context")
        print("   - Climate-aware feature selection")
        print("   - Lot-size specific prioritization")
        print("\nüí∞ API Cost Estimate: ~$0.01-0.02 per property")
        print("üéØ Expected Detection Improvement: 40-60% better relevance")
    else:
        print("\n‚ö†Ô∏è  Dynamic prompt generation: DISABLED (no API key)")
        print("   - Using default prompts as fallback")
        print("   - Set ANTHROPIC_API_KEY to enable")

    print("\n" + "=" * 100)


if __name__ == "__main__":
    test_prompt_agent()
